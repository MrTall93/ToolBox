# Docker Compose for LiteLLM + Tool Registry MCP Integration
# This setup allows you to run both services together for development and testing

version: '3.8'

services:
  # PostgreSQL with pgvector (shared database)
  postgres:
    image: pgvector/pgvector:pg16
    container_name: tool-registry-postgres
    environment:
      POSTGRES_DB: tool_registry
      POSTGRES_USER: tool_registry
      POSTGRES_PASSWORD: tool_registry_pass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U tool_registry -d tool_registry"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis for caching (LiteLLM)
  redis:
    image: redis:7-alpine
    container_name: litellm-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Tool Registry MCP Server
  tool-registry:
    build:
      context: .
      dockerfile: Dockerfile.ubi8
    container_name: tool-registry-server
    ports:
      - "8000:8000"
    environment:
      # Database configuration
      DATABASE_URL: "postgresql+asyncpg://tool_registry:tool_registry_pass@postgres:5432/tool_registry"

      # Application configuration
      APP_NAME: "tool-registry-mcp"
      DEBUG: "false"
      LOG_LEVEL: "INFO"

      # Embedding service configuration
      EMBEDDING_ENDPOINT_URL: "${EMBEDDING_ENDPOINT_URL:-https://api.openai.com/v1/embeddings}"
      EMBEDDING_API_KEY: "${EMBEDDING_API_KEY}"
      EMBEDDING_DIMENSION: "1536"

      # Security configuration
      API_KEY: "${TOOL_REGISTRY_API_KEY:-your-secret-api-key}"
      SECRET_KEY: "${SECRET_KEY:-your-secret-key-change-in-production}"

      # CORS configuration
      CORS_ORIGINS: "http://localhost:4000,http://localhost:3000,http://localhost:8080,http://localhost:8000"

      # Performance configuration
      DB_POOL_SIZE: "10"
      DB_MAX_OVERFLOW: "20"

      # Caching configuration
      ENABLE_CACHE: "true"
      CACHE_TTL: "300"

      # Embedding caching
      ENABLE_EMBEDDING_CACHE: "true"
      EMBEDDING_CACHE_SIZE: "1000"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      - ./examples:/app/examples:ro
    restart: unless-stopped

  # LiteLLM Server with MCP Gateway
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-server
    ports:
      - "4000:4000"      # LiteLLM API port
      - "9090:9090"      # Metrics port
    environment:
      # LiteLLM configuration
      # Load configuration from file
      CONFIG_PATH: "/app/config/litellm-mcp.yaml"

      # Model provider API keys
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
      ANTHROPIC_API_KEY: "${ANTHROPIC_API_KEY}"

      # Tool Registry MCP configuration
      TOOL_REGISTRY_API_KEY: "${TOOL_REGISTRY_API_KEY:-your-secret-api-key}"

      # Redis configuration
      REDIS_HOST: "redis"
      REDIS_PORT: "6379"
      REDIS_PASSWORD: "${REDIS_PASSWORD:-}"

      # General LiteLLM settings
      LITELM_LOG: "INFO"
      LITELM_REQUEST_TIMEOUT: "60"
      LITELM_NUM_RETRIES: "3"

      # Database for LiteLLM (optional, for usage tracking)
      DATABASE_URL: "postgresql://tool_registry:tool_registry_pass@postgres:5432/litellm"

      # Security settings
      LITELM_API_KEY: "${LITELM_API_KEY:-your-litellm-api-key}"
      LITELM_SALT_KEY: "${LITELM_SALT_KEY:-your-salt-key}"
    volumes:
      - ./config/litellm-mcp.yaml:/app/config/litellm-mcp.yaml:ro
    depends_on:
      - tool-registry
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Nginx reverse proxy (optional, for production)
  nginx:
    image: nginx:alpine
    container_name: litellm-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - litellm
      - tool-registry
    restart: unless-stopped
    profiles:
      - production

  # Prometheus for monitoring (optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: litellm-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    depends_on:
      - litellm
    restart: unless-stopped
    profiles:
      - monitoring

  # Grafana for visualization (optional)
  grafana:
    image: grafana/grafana:latest
    container_name: litellm-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  default:
    name: tool-registry-network