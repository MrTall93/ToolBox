apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: toolbox
data:
  config.yaml: |
    model_list:
      # Basic completion model for testing
      - model_name: "gpt-4o-mini"
        litellm_params:
          model: "openai/gpt-4o-mini"
          api_base: "https://api.openai.org/v1"

    # General settings
    litellm_settings:
      drop_params: true  # Drop unsupported params
      set_verbose: true

    # Enable MCP functionality
    mcp_settings:
      enabled: true
      auto_register: true  # Automatically accept tool registrations
      servers:
        # Toolbox FastMCP - Tool Registry
        - name: "toolbox-fastmcp"
          url: "http://toolbox-mcp-http:8080"
          description: "Tool Registry FastMCP Server - Unified tool management and execution"

        # Fetch MCP - Web fetching capabilities
        - name: "fetch"
          command: ["uvx", "mcp-server-fetch"]
          description: "Fetch web pages and extract content"

        # Filesystem MCP - File operations (read-only for safety)
        - name: "filesystem"
          command: ["npx", "-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
          description: "Read-only filesystem access to /tmp directory"

        # Memory MCP - Persistent memory/knowledge graph
        - name: "memory"
          command: ["npx", "-y", "@modelcontextprotocol/server-memory"]
          description: "Persistent memory and knowledge graph storage"

        # Sequential Thinking MCP - Step-by-step reasoning
        - name: "sequential-thinking"
          command: ["npx", "-y", "@modelcontextprotocol/server-sequential-thinking"]
          description: "Sequential thinking and reasoning capabilities"

    # Router settings (if using proxy mode)
    router_settings:
      routing_strategy: "usage-based"

    # Logging
    general_settings:
      master_key: "sk-12345"  # Default key for testing
      store_model_in_db: true  # Store MCP servers and tools in database